env_name: halfcheetah
env_maker: salina_examples.rl.ppo_brax.agents.make_brax_env
device: cuda:0
logger:
  classname: salina.logger.TFLogger
  logdir: ./experiments/halfcheetah
  cache_size: 10000
  modulo: 1
  verbose: False

acquisition:
  seed: 432
  n_timesteps: 20
  env:
    env_name: ${env_name}
    n_envs: 1024
    episode_length: 1000

validation:
  seed: 532
  evaluate_every: 10
  env:
    env_name: ${env_name}
    n_envs: 1
    episode_length: 1000

policy_agent:
  classname: salina_examples.rl.ppo_brax.agents.ActionAgent
  hidden_size: 64
  n_layers: 4
  env:
    classname: ${env_maker}
    env_name: ${env_name}

critic_agent:
  classname: salina_examples.rl.ppo_brax.agents.CriticAgent
  hidden_size: 256
  n_layers: 5
  env:
    classname: ${env_maker}
    env_name: ${env_name}

normalizer_agent:
  classname: salina_examples.rl.ppo_brax.agents.Normalizer
  device: ${device}
  env:
    classname: ${env_maker}
    env_name: ${env_name}

algorithm:
  clip_grad: 100
  update_epochs: 16
  minibatch_size: 512
  time_limit: 10000000
  max_epochs: 1501
  discount_factor: 0.99
  clip_ratio: 0.2
  action_std: 0.4
  gae: 0.96
  reward_scaling: 1
  lr_policy: 0.0003
  lr_critic: 0.0003
  normalize_obs: True


hydra:
  launcher:
    mem_gb: 16
    max_num_timeout: 0
    cpus_per_task: 1
    signal_delay_s: 30
    timeout_min: 60
    gpus_per_node: 1
    tasks_per_node: 1
    partition: learnlab
  job_logging:
    root:
      handlers: []

defaults:
  - override hydra/launcher: submitit_slurm