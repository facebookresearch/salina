name: sac_finetune
seed: 0

defaults:
  - logger: logger
  - scenario: ant_benchmark
  - hydra: hydra
  - override hydra/launcher: submitit_slurm

model:
  classname: salina_cl.models.models.Baseline
  seed: ${seed}
  params:
    checkpoint: False
    evaluation:
      device: cuda:0
      seed: ${seed}
      n_evaluation_envs: ${scenario.n_evaluation_envs}
      n_rollouts: 10
      evaluate_success: False
    algorithm: 
      classname: salina_cl.algorithms.sac.sac
      params:
        learning_device: cuda:0
        acquisition_device: cuda:0
        buffer_device: cuda:0
    
        optimizer_policy:
          classname: torch.optim.Adam
          lr: 3e-4
          weight_decay: 0.
    
        optimizer_q:
          classname: torch.optim.Adam
          lr: 3e-4

        optimizer_entropy:
          classname: torch.optim.Adam
          lr: 3e-4
    
        control_every_n_epochs: 100
        n_control_rollouts: 1
      
        n_processes: 0
  
        reward_scaling: 10.
        policy_update_delay: 4
        target_update_delay: 4
        time_limit: 0
        
        n_timesteps: 2
        batch_size: 256

        init_temperature: 2.
        target_multiplier: 2.
       
        clip_grad: 0.
        inner_epochs: ${scenario.n_train_envs}
        grad_updates_per_step: 0.5
        discount_factor: 0.99
        update_target_tau: 0.005
        buffer_time_size: 2
        buffer_size: 1_280_000
        initial_buffer_size: 12_800

    policy_agent:
      classname: salina_cl.agents.single_agents_sac.MultiActionAgent
      hidden_size: 256
      input_dimension: nil
      output_dimension: nil
      start_steps: 0

    critic_agent:
      classname: salina_cl.agents.single_agents_sac.TwinCritics
      hidden_size: 256
      obs_dimension: nil
      action_dimension: nil