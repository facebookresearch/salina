name: td3_finetune
seed: 0
perf_path: results/test.dat
save_model: False
final_evaluation: False

defaults:
  - logger: logger
  - scenario: cw3
  - hydra: hydra
  - override hydra/launcher: submitit_slurm

model:
  classname: salina_cl.models.models.OneStep
  seed: ${seed}
  params:
     
    evaluation:
      device: cpu
      seed: ${seed}
      n_rollouts: 5
      evaluate_success: True
    reset_critic_on_task_change: False
## params of the alg: https://github.com/awarelab/continual_world/blob/main/input_args.py
    algorithm: 
      classname: salina_cl.algorithms.td3.td3
      params:
        learning_device: cpu
        acquisition_device: cpu
        buffer_device: cpu
    
        optimizer_policy:
          classname: torch.optim.Adam
          lr: 0.0003  ## 3e-5 , 1e-4 , 3e-4, 1e-3
    
        optimizer_q:
          classname: torch.optim.Adam
          lr: 0.0003  ## 3e-5 , 1e-4 , 3e-4, 1e-3
    
        control_every_n_epochs: 100
        n_control_rollouts: 1
      
        n_processes: 1
    
        reward_scaling: 1.0
        policy_update_delay: 2 # 8
        time_limit: 0
        
        n_timesteps: 50  ## burning timesteps
        batch_size: 128   ## 128 ,256,512
        
        target_noise: 0.2
        action_noise: 0.4
        noise_clip: 0.5

        clip_grad: 50.
        inner_epochs: 50  ## 50
        discount_factor: 0.995  ## 0.99,0.995,0.95
        update_target_tau: 0.005
        # loss_device: cpu
        buffer_time_size: 2
        buffer_size: 1_000_000  ## 1e6
        initial_buffer_size: 10_000  ###  1e4
        
        ## check https://arxiv.org/pdf/2105.10919.pdf  page 15

    policy_agent:
      classname: salina_cl.agents.single_agents_td3.CWActionAgent     #ActionAgent
      hidden_size: 256
      n_layers: 4
      input_dimension: nil
      output_dimension: nil

    critic_agent:
      classname: salina_cl.agents.single_agents_td3.CWTwinCritics #CriticAgent
      hidden_size: 256
      n_layers: 4
      obs_dimension: nil
      action_dimension: nil