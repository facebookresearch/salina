name: sac_finetune
seed: 0
perf_path: results/test.dat
save_model: False
final_evaluation: False

defaults:
  - logger: logger
  - scenario: halfcheetah_benchmark
  - hydra: hydra
  - override hydra/launcher: submitit_slurm

model:
  classname: salina_cl.models.models.Baseline
  seed: ${seed}
  params:
    checkpoint: False
    evaluation:
      device: cuda:0
      seed: ${seed}
      n_evaluation_envs: ${scenario.n_evaluation_envs}
      n_rollouts: 10
      evaluate_success: False
    algorithm:
      classname: salina_cl.algorithms.sac.sac
      params:
        learning_device: cuda:0
        acquisition_device: cuda:0
        buffer_device: cuda:0

        optimizer_policy:
          classname: torch.optim.Adam
          lr: 1e-3 # 1e-3, 

        optimizer_q:
          classname: torch.optim.Adam
          lr: 3e-4 # 2e-3, 

        optimizer_entropy:
          classname: torch.optim.Adam
          lr: 3e-4

        control_every_n_epochs: 100
        n_control_rollouts: 1
      
        n_processes: 999999

        reward_scaling: 1. # 1, 10
        policy_update_delay: 2 # 2,4
        target_update_delay: 2 # 2,4
        time_limit: 0
        
        n_timesteps: 2
        batch_size: 256

        init_temperature: 2.
        target_multiplier: 2. # 1, 2

    policy_agent:
      classname: salina_cl.agents.single_agents_sac.MultiActionAgent
      hidden_size: 256
      input_dimension: nil
      output_dimension: nil
      start_steps: 0
      layer_norm: False

    critic_agent:
      classname: salina_cl.agents.single_agents_sac.TwinCritics
      hidden_size: 256
      obs_dimension: nil
      action_dimension: nil

